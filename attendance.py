# -*- coding: utf-8 -*-
"""Attendance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zJLLO-z1XCwAPKLrRx2lDnAjWjJcN55q
"""

!pip install face_recognition
import face_recognition

import os
import cv2
import numpy as np
from datetime import datetime
from google.colab.patches import cv2_imshow

from google.colab import drive
drive.mount('/content/drive')

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

#find images from drive
path = '/content/drive/My Drive/Images'
images = []
classNames = []
myList = os.listdir(path)
print(myList)
for cl in myList:
  curImg = cv2.imread(f'{path}/{cl}')
  images.append(curImg)
  classNames.append(os.path.splitext(cl)[0])
print(classNames)

#mark attendance
def markAttendance(name):
  with open('Attendance.csv','r+') as f:
    myDataList = f.readlines()
    nameList =[]
    for line in myDataList:
      entry = line.split(',')
      nameList.append(entry[0])
    if name not in nameList:
      now = datetime.now()
      dtString = now.strftime('%H:%M:%S')
      f.writelines(f'\n{name},{dtString}')

#compute encodings for images
def findEncodings(images):
  encodeList = []
  for img in images:
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    encode = face_recognition.face_encodings(img)[0]
    encodeList.append(encode)
  return encodeList

encodeListKnown = findEncodings(images)
print('Encoding Complete')

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))

  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

#find matches
imgS = face_recognition.load_image_file('photo.jpg')
imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)

facesCurFrame = face_recognition.face_locations(imgS)
encodesCurFrame = face_recognition.face_encodings(imgS, facesCurFrame)

for encodeFace, faceLoc in zip(encodesCurFrame, facesCurFrame):
  matches = face_recognition.compare_faces(encodeListKnown, encodeFace)
  faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)
  #print(faceDis)
  matchIndex = np.argmin(faceDis)

  if matches[matchIndex]:
    name = classNames[matchIndex].upper()
    #print(name)
    y1, x2, y2, x1 = faceLoc
    cv2.rectangle(imgS, (x1, y1), (x2, y2), (0, 255, 0), 2)
    cv2.rectangle(imgS, (x1, y2-35),(x2,y2),(0,255,0), cv2.FILLED)
    cv2.putText(imgS, name, (x1+6, y2-6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)
    markAttendance(name)

cv2_imshow(imgS)
cv2.waitKey(1)